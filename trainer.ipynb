{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 로컬에서 학습시키는 스크립트\n",
    "\n",
    "* 데이터 manipulation 작업도 하는 곳\n",
    "* 학습에 관한 여러가지 테스트도 수행할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    \n",
    "    def __init__(self, h, w, sess):\n",
    "        self.size_h = h\n",
    "        self.size_w = w\n",
    "        self.sess = sess\n",
    "        self.model = self.make_model()\n",
    "        \n",
    "    def make_model(self):\n",
    "        \n",
    "        # žðµš\n",
    "        self.observation = tf.placeholder(shape=[None, self.size_h, self.size_w, 1], dtype=tf.float32) # ÀÌ¹ÌÁö µ¥ÀÌÅÍ\n",
    "        self.label = tf.placeholder(shape=[None, 3], dtype=tf.int16) # ¶óº§ µ¥ÀÌÅÍ\n",
    "        # L1 ImgIn shape=(?,self.size_h, self.size_w,1)\n",
    "        self.w_in = tf.Variable(tf.random_normal([5,5,1,8], stddev=.01)) # conv 1 °¡ÁßÄ¡\n",
    "        self.l1 = tf.nn.conv2d(self.observation, self.w_in, strides=[1,1,1,1], padding='SAME')\n",
    "        #    Conv     -> (?, self.size_h, self.size_w, 8)\n",
    "        self.l1 = tf.nn.relu(self.l1)\n",
    "        self.l1 = tf.nn.max_pool(self.l1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "        #    Pool     -> (?, self.size_h/2, self.size_w/2, 8)\n",
    "        \n",
    "        # L2 ImgIn shape=(?,self.size_h/2, self.size_w/2,8)\n",
    "        self.w2_in = tf.Variable(tf.random_normal([5,5,8,16], stddev=.01)) # conv 2 °¡ÁßÄ¡\n",
    "        self.l2 = tf.nn.conv2d(self.l1, self.w2_in, strides=[1,1,1,1], padding='SAME')\n",
    "        #    Conv     -> (?, self.size_h/2, self.size_w/2, 16)\n",
    "        self.l2 = tf.nn.relu(self.l2)\n",
    "        self.l2 = tf.nn.max_pool(self.l2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "        #    Pool     -> (?, self.size_h/4, self.size_w/4, 16)\n",
    "           \n",
    "        '''\n",
    "        # L3 ImgIn shape=(?,self.size_h/4, self.size_w/4,16)\n",
    "        self.w3_in = tf.Variable(tf.random_normal([5,5,8,16], stddev=.01)) # conv 2 °¡ÁßÄ¡\n",
    "        self.l3 = tf.nn.conv2d(self.l2, self.w3_in, strides=[1,1,1,1], padding='SAME')\n",
    "        #    Conv     -> (?, self.size_h/4, self.size_w/4, 16)\n",
    "        self.l3 = tf.nn.relu(self.l3)\n",
    "        self.l3 = tf.nn.max_pool(self.l3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "        #    Pool     -> (?, self.size_h/8, self.size_w/8, 16)\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        self.w_out = tf.Variable(tf.random_normal(shape=[self.size_w//4*self.size_h//4*16, 3], stddev=0.01))\n",
    "        self.b = tf.Variable(tf.random_normal([3]))\n",
    "        self.h_flat = tf.reshape(self.l2, [-1, self.size_w//4*self.size_h//4*16])\n",
    "        self.output = tf.matmul(self.h_flat, self.w_out) + self.b\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.output, labels=self.label))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=.0005).minimize(self.cost)\n",
    "        print('> žðµš »ýŒº ¿Ï·á')\n",
    "        \n",
    "        \n",
    "    def train(self, batch_in, batch_label):\n",
    "        # ÇÐœÀœÃÇà.\n",
    "        _, cost = self.sess.run([self.optimizer, self.cost], feed_dict={self.observation:batch_in, self.label:batch_label})\n",
    "        return cost\n",
    "    \n",
    "    def policy(self, input_data):\n",
    "        # policy žž µµÃâÇÏ±â\n",
    "        output = self.sess.run(self.output, feed_dict={self.observation:input_data})\n",
    "        return output\n",
    "    \n",
    "    def test(self, in_test, label_test):\n",
    "        # Å×œºÆ® µ¥ÀÌÅÍ ŒÂÀ» ¹Ìž® ÀúÀåÇØ³õ°í °ËÁõÇÏŽÂ žÞŒ­µå\n",
    "        in_test_copied = copy.deepcopy(in_test)\n",
    "        cnt = 0\n",
    "        for step in range(len(in_test_copied)):\n",
    "            key = cv2.waitKey(10) & 0xFF\n",
    "            if(key == ord('q')):\n",
    "                break\n",
    "            out = self.sess.run(self.output, feed_dict={self.observation:in_test_copied[step:step+1]})\n",
    "            idx = np.argmax(out, axis=1)\n",
    "            one_hot = np.zeros_like(out)\n",
    "            one_hot[0][idx[0]] = 1\n",
    "            '''if(np.all(one_hot == label_test[])):\n",
    "                cnt += 1'''\n",
    "            cv2.putText(in_test_copied[step], 'command : {}'.format(one_hot[0]), (10, 30), cv2.FONT_HERSHEY_COMPLEX, .3, (0,0,255), 1)\n",
    "            cv2.imshow('testing..', in_test_copied[step])\n",
    "        cv2.destroyAllWindows()\n",
    "        accuracy = cnt / len(in_test)\n",
    "        print('acc :', accuracy)\n",
    "        \n",
    "    def test_live(self):\n",
    "        # œÇœÃ°£Àž·Î ±æÀ» Àß ÀÎœÄÇÏŽÂÁö Å×œºÆ®ÇÏŽÂ žÞŒ­µå.\n",
    "        out = self.sess.run(self.output, feed_dict={self.observation:d})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 forming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 사전학습 데이터 1016 frame, load & setting 완료\n",
      "(1016, 320, 640, 4)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('dataset.h5', 'r') as f: # binary 파일로 학습데이터 다루기\n",
    "    batch_in = np.array(f['observation'])\n",
    "    batch_in = np.expand_dims(batch_in, axis=3)\n",
    "    batch_in.dtype = np.uint8\n",
    "    batch_label = np.array(f['label'])\n",
    "    print('> 사전학습 데이터 {} frame, load & setting 완료'.format(len(batch_in)))\n",
    "    print(batch_in.shape)\n",
    "    #print(list(batch_in[10]), batch_label[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추가 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 추가학습 데이터 103 frame, load & setting 완료\n",
      "(103, 320, 640, 4)\n",
      "> 편집 후 frame :  80\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('dataset_new.h5', 'r') as f:\n",
    "    batch_in_new = np.array(f['observation'])\n",
    "    batch_in_new = np.expand_dims(batch_in_new, axis=3)\n",
    "    batch_in_new.dtype = np.uint8\n",
    "    batch_label_new = np.array(f['label'])\n",
    "    print('> 추가학습 데이터 {} frame, load & setting 완료'.format(len(batch_in_new)))\n",
    "    print(batch_in_new.shape)\n",
    "    batch_in_new = list(batch_in_new[:52]) + list(batch_in_new[59:61]) + list(batch_in_new[64:90])\n",
    "    batch_label_new = list(batch_label_new[:52]) + list(batch_label_new[59:61]) + list(batch_label_new[64:90])\n",
    "    print('> 편집 후 frame : ', len(batch_in_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 보강 - 데이터 합칠 때만 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1096, 320, 640, 4)\n",
      "> 최종 데이터 1096 frame, 생성 완료\n"
     ]
    }
   ],
   "source": [
    "batch_in_aggre = list(batch_in) + list(batch_in_new)\n",
    "batch_label_aggre = list(batch_label) + list(batch_label_new)\n",
    "\n",
    "batch_in = np.array(batch_in_aggre)\n",
    "batch_label = np.array(batch_label_aggre)\n",
    "\n",
    "print(batch_in.shape)\n",
    "print('> 최종 데이터 {} frame, 생성 완료'.format(len(batch_in)))\n",
    "#print(list(batch_in[10]), batch_label[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추가된 데이터를 그대로 파일에 저장 - 필요할 때만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "with h5py.File('dataset.h5', 'r') as f:\n",
    "    del(f['observation'])\n",
    "    del(f['label'])\n",
    "    f['observation'] = batch_in_aggre\n",
    "    f['label'] = batch_label_aggre\n",
    "    \n",
    "    print('> 데이터 추가하여 {} frame, 저장 완료'.format(len(batch_in_aggre)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 디버깅 및 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(batch_in_new)):\n",
    "    #print(batch_label[idx+1016])\n",
    "    cv2.putText(batch_in_new[idx], str(batch_label_new[idx]) + ' ' + str(idx), (50,50), cv2.FONT_HERSHEY_SCRIPT_SIMPLEX, 1, (255,255,255), 2)\n",
    "    cv2.imshow('s', batch_in_new[idx])\n",
    "    if(cv2.waitKey(0) == ord('q')):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('dataset_new.h5', 'r') as f:\n",
    "    batch_in_new = np.array(f['observation'])\n",
    "    batch_in_new = np.expand_dims(batch_in_new, axis=3)\n",
    "    batch_in_new.dtype = np.uint8\n",
    "    batch_label_new = np.array(f['label'])\n",
    "    \n",
    "    batch_in_new = list(batch_in_new[:52]) + list(batch_in_new[59:61]) + list(batch_in_new[64:90])\n",
    "    batch_label_new = list(batch_label_new[:52]) + list(batch_label_new[59:61]) + list(batch_label_new[64:90])\n",
    "    print(len(batch_in_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 새로 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 모델 생성 완료\n",
      "> 데이터 학습 중..\n",
      "1/10 epoch, 손실크기 : 21.85662841796875, 118.28120422363281 초 소요\n",
      "2/10 epoch, 손실크기 : 114.87541961669922, 99.36302947998047 초 소요\n",
      "3/10 epoch, 손실크기 : 143.36880493164062, 94.52641296386719 초 소요\n",
      "4/10 epoch, 손실크기 : 21.604408264160156, 103.03260660171509 초 소요\n",
      "5/10 epoch, 손실크기 : 2.703319787979126, 88.62847709655762 초 소요\n",
      "6/10 epoch, 손실크기 : 1.826216697692871, 91.45965623855591 초 소요\n",
      "7/10 epoch, 손실크기 : 1.0754129886627197, 86.58218693733215 초 소요\n",
      "8/10 epoch, 손실크기 : 0.9419257640838623, 90.66830825805664 초 소요\n",
      "9/10 epoch, 손실크기 : 0.98337322473526, 83.32266998291016 초 소요\n",
      "10/10 epoch, 손실크기 : 0.9921904802322388, 91.43422293663025 초 소요\n"
     ]
    }
   ],
   "source": [
    "width = 320\n",
    "height = 640\n",
    "total_epoch = 10\n",
    "save_path = ''\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    start = time.time()\n",
    "    model = CNN(width, height, sess)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    print('> 데이터 학습 중..')\n",
    "    for epoch in range(total_epoch):\n",
    "        start_epoch = time.time()\n",
    "        cost = model.train(batch_in, batch_label)\n",
    "        print('{}/{} epoch, 손실크기 : {}, {} 초 소요'.format(epoch+1, total_epoch, cost, time.time() - start_epoch))\n",
    "        \n",
    "    save_path = 'C:\\\\model'\n",
    "    saver.save(sess, save_path) # 모델 저장\n",
    "    took_time = time.time() - start\n",
    "    print('> 학습완료, {} 초 소요, 모델의 파라미터가 {} 에 저장됨.'.format(took_time, save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추가 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 320\n",
    "height = 640\n",
    "total_epoch = 10\n",
    "save_path = ''\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    start = time.time()\n",
    "    model = CNN(width, height, sess)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, save_path)\n",
    "    print('> 파라미터 load 완료')\n",
    "    print('> 추가 학습 시작..')\n",
    "    \n",
    "    for epoch in range(total_epoch):\n",
    "        start_epoch = time.time()\n",
    "        cost = model.train(batch_in, batch_label)\n",
    "        print('{}/{} epoch, 손실크기 : {}, {} 초 소요'.format(epoch+1, total_epoch, cost, time.time() - start_epoch))\n",
    "        \n",
    "    save_path = 'C:\\\\model'\n",
    "    saver.save(sess, save_path) # 모델 저장\n",
    "    took_time = time.time() - start\n",
    "    print('> 학습완료, {} 초 소요, 모델의 파라미터가 {} 에 저장됨.'.format(took_time, save_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
