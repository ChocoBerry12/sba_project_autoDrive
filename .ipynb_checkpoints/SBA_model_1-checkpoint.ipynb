{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imitation Learning 모델 1\n",
    "---\n",
    "## Description\n",
    "> SBA 5 조 스마트팩토리 자율주행을 위한 학습 프로그램. vanilla DAgger 모델. \n",
    "\n",
    "## todo\n",
    "* [x] 모듈 1 구현\n",
    "* [ ] 전처리 방법 다양화\n",
    "* [x] 모듈 2 구현\n",
    "* [ ] 모듈 3 구현\n",
    "* [ ] pickle 등으로 로컬에 데이터 저장\n",
    "\n",
    "## 링크\n",
    "* [PEP8 코딩준수](https://kongdols-room.tistory.com/18)\n",
    "* [openCV 가이드](https://opencv-python.readthedocs.io/en/latest/doc/02.videoStart/videoStart.html)\n",
    "* [프로그레스 바 구현](https://wikidocs.net/13977)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 학습데이터 생성 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class DataCollector:\\n    def __init__(self):\\n        pass'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''class DataCollector:\n",
    "    def __init__(self):\n",
    "        pass'''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProducer:\n",
    "    def __init__(self, width=320, height=240):\n",
    "        self.observation = [] # 전처리된 영상 데이터 np 원본\n",
    "        self.label = [] # 라벨링 데이터, observation 과 같은 크기\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        print('> data producer 생성. ')\n",
    "        print('> 가로 : {}, 세로 : {}'.format(width, height))\n",
    "        print('-'*50)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return '가로 : {}, 세로 : {}'.format(self.width, self.height)\n",
    "    \n",
    "    def record_video(self):\n",
    "        # 파일로부터 전처리 데이터 생성을 위하여 원본영상을 저장하는 녹화하는 메서드\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'MP42')\n",
    "        fps = 25.0\n",
    "        out = cv2.VideoWriter('recorded_1.avi', fourcc, fps, (self.width, self.height))\n",
    "        \n",
    "        cap = cv2.VideoCapture(0)\n",
    "        cap.set(3, self.width)\n",
    "        cap.set(4, self.height)\n",
    "        cnt = 0\n",
    "        while(cap.isOpened()):\n",
    "            cnt += 1\n",
    "            ret, frame = cap.read()\n",
    "            if(ret):\n",
    "                out.write(frame)\n",
    "                cv2.imshow('frame', frame)\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print('> {} 프레임 원본 녹화 완료'.format(cnt))\n",
    "        print('-'*50)\n",
    "    \n",
    "    def produce_observation(self, file=None, delay=10):\n",
    "        # observation 을 생성하는 메서드\n",
    "        self.observation = []\n",
    "        if(file == None):\n",
    "            # 재생할 원본영상이 없으면 실시간으로 카메라 영상 재생\n",
    "            # 여기서는 노트북의 로컬카메라\n",
    "            cap = cv2.VideoCapture(0)\n",
    "            cap.set(3, self.width)\n",
    "            cap.set(4, self.height)\n",
    "        else:\n",
    "            # 파일로부터 원본영상을 받아 재생\n",
    "            # 처음부터 전처리한 영상을 안만드는 이유는\n",
    "            # 하나의 원본으로부터 다양한 전처리 작업을 한 결과을 얻기 위함\n",
    "            cap = cv2.VideoCapture(file)\n",
    "        \n",
    "        while(cap.isOpened()):\n",
    "            ret, frame = cap.read()\n",
    "            #lower_black = np.array([0,0,0], dtype=np.uint8)\n",
    "            #upper_black = np.array([180,255,70], dtype=np.uint8)\n",
    "            \n",
    "            if(ret):\n",
    "                #hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "                #mask = cv2.inRange(hsv, lower_black, upper_black)\n",
    "                #res = cv2.bitwise_and(frame,frame, mask=mask)\n",
    "                \n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                ret_bin, binary = cv2.threshold(gray, 80, 255, cv2.THRESH_BINARY)\n",
    "                #blur = cv2.GaussianBlur(binary, (15, 15), 2)\n",
    "                \n",
    "                #cv2.imshow('frame', gray)\n",
    "                cv2.imshow('masked', binary)\n",
    "                self.observation.append(binary)\n",
    "                \n",
    "                if cv2.waitKey(delay) & 0xFF == ord('q'):\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        \n",
    "        # 영상을 뒤집어서 데이터 추가하기 - 실험용#\n",
    "        copied = copy.deepcopy(self.observation)\n",
    "        for i in range(len(copied)):\n",
    "            self.observation.append(cv2.flip(copied[i], 1))\n",
    "        for i in range(len(copied)):\n",
    "            self.observation.append(copied[i])\n",
    "            \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print('> observation 데이터 생성완료, 원본 프레임 수 : {}'.format(len(self.observation)))\n",
    "        print('-'*50)\n",
    "    \n",
    "    def record_processed(self):\n",
    "        # 전처리된 데이터를 다시 영상파일로 저장\n",
    "        # deprecated\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'MP42')\n",
    "        fps = 25.0\n",
    "        out = cv2.VideoWriter('processed_1.avi', fourcc, fps, (self.width,self.height), 0) # 세밀한 컨트롤을 위해 프레임 늘리기 고려\n",
    "        copied_observation = copy.deepcopy(self.observation)\n",
    "        for f in range(len(copied_observation)):\n",
    "            frame = copied_observation[f]\n",
    "            out.write(frame)\n",
    "        out.release()\n",
    "        print('> {} 프레임 전처리 영상 파일 저장 완료.'.format(len(copied_observation)))\n",
    "        print('-'*50)\n",
    "        \n",
    "    def produce_label(self, delay=10):\n",
    "        # 수동으로 라벨링하는 과정을 observation 구하는 것과 분리함.\n",
    "        # 다양한 라벨링을 테스트하기 위함\n",
    "        # one_hot encoding 형태\n",
    "        self.label = []\n",
    "        copied_observation = copy.deepcopy(self.observation)\n",
    "        command = [0,1,0]\n",
    "        for step in range(len(copied_observation)):\n",
    "            key = cv2.waitKey(10) & 0xFF\n",
    "            if(key == ord('q')):\n",
    "                break\n",
    "            elif(key == ord('a')):\n",
    "                command = [1,0,0]\n",
    "            elif(key == ord('d')):\n",
    "                command = [0,0,1]\n",
    "            elif(key == ord('w')):\n",
    "                command = [0,1,0]\n",
    "            self.label.append(command)\n",
    "            cv2.putText(copied_observation[step], 'command : {}'.format(command), (10, 30), cv2.FONT_HERSHEY_COMPLEX, .3, (0,0,255), 1)\n",
    "            cv2.imshow('labeling..', copied_observation[step])\n",
    "            \n",
    "        cv2.destroyAllWindows()\n",
    "        print('> label 데이터 생성완료, label 수 : {}'.format(len(self.label)))\n",
    "        print('-'*50)\n",
    "\n",
    "    def get_observation(self):\n",
    "        # 외부에서 영상을 만들기위해 원본을 제공\n",
    "        video_data = copy.deepcopy(self.observation)\n",
    "        print('> 전처리 영상 복사완료 - 프레임 수: {}'.format(len(self.observation)))\n",
    "        print('-'*50)\n",
    "        return video_data\n",
    "    \n",
    "    def get_label(self):\n",
    "        # 라벨 데이터를 제공하는 메서드\n",
    "        label_data = copy.deepcopy(self.label)\n",
    "        print('> 라벨 데이터 복사완료 - 라벨 수: {}'.format(len(self.label)))\n",
    "        print('-'*50)\n",
    "        return label_data\n",
    "    \n",
    "    def get_database(self):\n",
    "        # observation - label 데이터 쌍을 data 학습데이터로 생성하는 메서드\n",
    "        # CNN 에 사용하기 위해 4 dim 으로 reshape 한 후 최종 학습데이터 반환\n",
    "        print('> 영상 프레임 수 : {}, 라벨 프레임 수 : {}, 부족한 라벨 수: {}'.format(len(self.observation), len(self.label), len(self.observation) - len(self.label)))\n",
    "        print('> {} 프레임 학습 데이터 생성완료.'.format(len(self.label)))\n",
    "        print('-'*50)\n",
    "        selected_ob = self.observation[:len(self.label)]\n",
    "        selected_ob = np.array(selected_ob)\n",
    "        selected_ob = selected_ob.reshape([-1,self.height,self.width,1])\n",
    "        return zip(selected_ob, self.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> data producer 생성. \n",
      "> 가로 : 160, 세로 : 120\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dp = DataProducer(width=160, height=120) # 학습데이터 생성기 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 750 프레임 원본 녹화 완료\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dp.record_video() # 결과는 recorded_1.avi 로 저장. 원본을 따로 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> observation 데이터 생성완료, 원본 프레임 수 : 2250\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dp.produce_observation('recorded_1.avi', delay=1) # 파일로부터 전처리영상 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> observation 데이터 생성완료, 원본 프레임 수 : 39\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dp.produce_observation() # raw 영상으로부터 전처리영상 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 1150 프레임 전처리 영상 파일 저장 완료.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#dp.record_processed() # observation 데이터를 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> label 데이터 생성완료, label 수 : 2250\n",
      "--------------------------------------------------\n",
      "> 영상 프레임 수 : 2250, 라벨 프레임 수 : 2250, 부족한 라벨 수: 0\n",
      "> 2250 프레임 학습 데이터 생성완료.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dp.produce_label(delay=10) # 만들어진 동영상파일을 가지고 수동으로 라벨링하여 라벨을 프레임별로 따로 저장\n",
    "data = list(dp.get_database()) # observation - label 모음 학습데이터를 획득한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 신경망 학습 모듈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorflow 관련\n",
    "* feed 데이터는 기본 자료형, numpy 가능. tensor 는 순수한 값이 아닌 연산노드이므로 계산불가\n",
    "* `squeeze()`, `expand_dim()` 으로 dimension 조절\n",
    "* one-hot 인코딩 데이터 배열을 비교할 때는 `np.all(a==b)`\n",
    "* logit 과 sigmoid, softmax 의 관계는 역함수 관계.\n",
    "* `softmax_cross_entropy_with_logits_v2` 는 네트워크의 출력인 logit 을 그대로 때려박아서 분류결과를 내놓는다.\n",
    "* softmax(logit) 하고 corssentropy 를 적용하는 작업을 합친것임. logit 은 softmax 의 역함수이므로\n",
    "---\n",
    "### TODO\n",
    "* [ ] 네트워크 변형하며 성능 테스트\n",
    "* [ ] label 데이터의 형태 변형하여 테스트 - 현재는 3 개 카테고리의 분류문제인데 너무 단순하고 practical 하지 않음\n",
    "* [ ] 실제로는 steering angle 에 대응하는 floating number 로 regression output 이 필요\n",
    "* [ ] 검증하는 코드 작성하기\n",
    "* [ ] CPU 점유율 너무높음. AWS 쓰게 해줘 ㅠㅠ\n",
    "* Imitation Learning 을 위해 output 은 더 세밀한 action 이 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self, h, w, sess):\n",
    "        self.size_h = h\n",
    "        self.size_w = w\n",
    "        self.sess = sess\n",
    "        self.model = self.make_model()\n",
    "    \n",
    "    def make_model(self):\n",
    "        # 모델\n",
    "        self.observation = tf.placeholder(shape=[None, self.size_h, self.size_w, 1], dtype=tf.float32) # 이미지 데이터\n",
    "        self.label = tf.placeholder(shape=[None, 3], dtype=tf.int16) # 라벨 데이터\n",
    "        \n",
    "        self.w_in = tf.Variable(tf.random_normal([5,5,1,8], stddev=.01)) # conv 1 가중치\n",
    "        self.l1 = tf.nn.conv2d(self.observation, self.w_in, strides=[1,1,1,1], padding='SAME')\n",
    "        self.l1 = tf.nn.relu(self.l1)\n",
    "        self.l1 = tf.nn.max_pool(self.l1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "        \n",
    "        self.w_out = tf.Variable(tf.random_normal(shape=[self.size_w//2*self.size_h//2*8, 3], stddev=.01))\n",
    "        self.b = tf.Variable(tf.random_normal([3]))\n",
    "        self.h_flat = tf.reshape(self.l1, [-1, self.size_w//2*self.size_h//2*8])\n",
    "        \n",
    "        self.output = tf.matmul(self.h_flat, self.w_out) + self.b\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.output, labels=self.label))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=.001).minimize(self.cost)\n",
    "        print('> 모델 생성 완료')\n",
    "        \n",
    "    def train(self, batch_in, batch_label):\n",
    "        # 학습시행.\n",
    "        _, cost = self.sess.run([self.optimizer, self.cost], feed_dict={self.observation:batch_in, self.label:batch_label})\n",
    "        return cost\n",
    "    \n",
    "    def test(self, in_test, label_test):\n",
    "        # 길을 잘 인식하는지 테스트.\n",
    "        # 테스트 데이터 셋을 미리 저장해놓고 검증하는 메서드\n",
    "        in_test_copied = copy.deepcopy(in_test)\n",
    "        cnt = 0\n",
    "        for step in range(len(in_test_copied)):\n",
    "            key = cv2.waitKey(10) & 0xFF\n",
    "            if(key == ord('q')):\n",
    "                break\n",
    "            out = self.sess.run(self.output, feed_dict={self.observation:in_test_copied[step:step+1]})\n",
    "            idx = np.argmax(out, axis=1)\n",
    "            one_hot = np.zeros_like(out)\n",
    "            one_hot[0][idx[0]] = 1\n",
    "            '''if(np.all(one_hot == label_test[])):\n",
    "                cnt += 1'''\n",
    "            cv2.putText(in_test_copied[step], 'command : {}'.format(one_hot[0]), (10, 30), cv2.FONT_HERSHEY_COMPLEX, .3, (0,0,255), 1)\n",
    "            cv2.imshow('testing..', in_test_copied[step])\n",
    "        \n",
    "        cv2.destroyAllWindows()\n",
    "        accuracy = cnt / len(in_test)\n",
    "        print('acc :', accuracy)\n",
    "    \n",
    "    def test_live(self):\n",
    "        # 실시간으로 길을 잘 인식하는지 테스트하는 메서드.\n",
    "        out = self.sess.run(self.output, feed_dict={self.observation:d})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 총 데이터 갯수 : 2250, 학습 데이터 갯수 : 1575, 검증 데이터 갯수 : 675\n",
      "start\n",
      "> 모델 생성 완료\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-804b55480fbf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_in_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_label_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{} epoch, cost : {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-79-d799f3f0859d>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, batch_in, batch_label)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m# 학습시행.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_label\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 하이퍼 파라미터\n",
    "learning_rate = .001\n",
    "total_epoch = 5\n",
    "\n",
    "# train data, test data 절반씩 나누기\n",
    "batch_in_train = [data[i][0] for i in range(int(len(data)*.7))]\n",
    "batch_label_train = [data[i][1] for i in range(int(len(data)*.7))]\n",
    "batch_in_test = [data[i][0] for i in range(len(data)-len(batch_in_train))]\n",
    "batch_label_test = [data[i][1] for i in range(len(data)-len(batch_in_train))]\n",
    "\n",
    "print('> 총 데이터 갯수 : {}, 학습 데이터 갯수 : {}, 검증 데이터 갯수 : {}'.format(len(data), len(batch_in_train), len(batch_in_test)))\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print('start')\n",
    "    model = CNN(120, 160, sess)\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for epoch in range(total_epoch):\n",
    "        cost = model.train(batch_in_train, batch_label_train)\n",
    "        print('{} epoch, cost : {}'.format(epoch+1, cost))\n",
    "    input()\n",
    "    model.test(batch_in_test, batch_label_test) # video 를 띄워 검증한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input()\n",
    "print('aa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 실시간 Data Aggregatioin 모듈"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
